<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Yi Liu is a Senior Technical Manager at Baidu PaddlePaddle, leading large model (LLM/VLM) initiatives across the AI stack, with research interests in computer vision and vision-language models.">
    <meta name="robots" content="index,follow">
    <link rel="canonical" href="https://liuyi.ai/">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:site_name" content="Yi Liu">
    <meta property="og:title" content="Yi Liu - Senior Technical Manager, PaddlePaddle">
    <meta property="og:description" content="Senior Technical Manager at Baidu PaddlePaddle. Leading large model (LLM/VLM) initiatives across the AI stack.">
    <meta property="og:url" content="https://liuyi.ai/">
    <meta property="og:image" content="https://liuyi.ai/profile.jpg">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Yi Liu - Senior Technical Manager, PaddlePaddle">
    <meta name="twitter:description" content="Senior Technical Manager at Baidu PaddlePaddle. Leading large model (LLM/VLM) initiatives across the AI stack.">
    <meta name="twitter:image" content="https://liuyi.ai/profile.jpg">

    <!-- Browser UI -->
    <meta name="theme-color" content="#0a0e27">
    <meta name="theme-color" content="#f8fafc" media="(prefers-color-scheme: light)">

    <!-- Structured Data -->
    <script type="application/ld+json">
        {
            "@context": "https://schema.org",
            "@type": "Person",
            "name": "Yi Liu",
            "alternateName": "åˆ˜æ¯…",
            "url": "https://liuyi.ai/",
            "image": "https://liuyi.ai/profile.jpg",
            "jobTitle": "Senior Technical Manager",
            "worksFor": {
                "@type": "Organization",
                "name": "Baidu"
            },
            "affiliation": {
                "@type": "Organization",
                "name": "PaddlePaddle Team, Baidu"
            },
            "alumniOf": [
                {
                    "@type": "CollegeOrUniversity",
                    "name": "Nanyang Technological University"
                },
                {
                    "@type": "CollegeOrUniversity",
                    "name": "Harbin Institute of Technology"
                }
            ],
            "sameAs": [
                "https://github.com/aigcliu",
                "https://scholar.google.com/citations?user=d_wa7ogAAAAJ&hl=en",
                "https://www.linkedin.com/in/liuyi1990/"
            ]
        }
    </script>
    <title>Yi Liu - Senior Technical Manager, PaddlePaddle</title>
    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="alternate icon" href="favicon.svg" type="image/svg+xml">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- é¡¶éƒ¨å¯¼èˆªæ  -->
    <header class="header" role="banner">
        <div class="header-content">
            <h1 class="header-name">Yi Liu (<span lang="zh-Hans">åˆ˜æ¯…</span>)</h1>
            <nav class="nav" role="navigation" aria-label="Main navigation">
                <a href="#about" class="nav-link" aria-label="Navigate to About section">About</a>
                <a href="#news" class="nav-link" aria-label="Navigate to News section">News</a>
                <a href="#projects" class="nav-link" aria-label="Navigate to Projects section">Projects</a>
                <a href="#publications" class="nav-link" aria-label="Navigate to Publications section">Publications</a>
                <!-- ä¸»é¢˜åˆ‡æ¢æŒ‰é’® -->
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme" title="Toggle theme">
                    <svg id="sunIcon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                    <svg id="moonIcon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                </button>
            </nav>
        </div>
    </header>

    <!-- ä¸»å®¹å™¨ -->
    <div class="main-container">
        <!-- å·¦ä¾§è¾¹æ  -->
        <aside class="sidebar" role="complementary" aria-label="Author information">
            <div class="sidebar-content">
                <!-- ä¸ªäººç…§ç‰‡ -->
                <div class="avatar-wrapper">
                    <img src="profile.jpg" alt="Yi Liu" class="avatar" onerror="this.style.display='none'; this.nextElementSibling.style.display='flex';">
                    <div class="avatar-placeholder">
                        <svg viewBox="0 0 24 24" width="60" height="60">
                            <path fill="currentColor" d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"/>
                        </svg>
                    </div>
                </div>

                <!-- åŸºæœ¬ä¿¡æ¯ -->
                <h2 class="sidebar-name">Dr. Yi Liu</h2>
                <p class="sidebar-title">Senior Technical Manager</p>
                <p class="sidebar-affiliation">
                    PaddlePaddle Team, Baidu<br>
                
                </p>

                <!-- è”ç³»æ–¹å¼ -->
                <div class="contact-info">
                    <div class="contact-item">
                        <strong>Email:</strong>
                        <span>liuyi.ntu AT gmail.com</span>
                    </div>
                    <div class="contact-item">
                        <strong>Location:</strong>
                        <span>Shenzhen, Beijing, Shanghai</span>
                    </div>
                </div>

                <!-- ç¤¾äº¤é“¾æŽ¥ -->
                <div class="social-links">
                    <a href="mailto:liuyi.ntu@gmail.com" class="social-link" title="Email" aria-label="Send email to Yi Liu">
                        <svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true">
                            <path fill="currentColor" d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/>
                        </svg>
                    </a>
                    <a href="https://github.com/aigcliu" target="_blank" rel="noopener noreferrer" class="social-link" title="GitHub" aria-label="Visit Yi Liu's GitHub profile">
                        <svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true">
                            <path fill="currentColor" d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                        </svg>
                    </a>
                    <a href="https://scholar.google.com/citations?user=d_wa7ogAAAAJ&hl=en" target="_blank" rel="noopener noreferrer" class="social-link" title="Google Scholar" aria-label="Visit Yi Liu's Google Scholar profile">
                        <svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true">
                            <path fill="currentColor" d="M12 24a7 7 0 110-14 7 7 0 010 14zm0-24L0 9.5l4.838 3.94A8 8 0 0112 9a8 8 0 017.162 4.44L24 9.5z"/>
                        </svg>
                    </a>
                    <a href="https://www.linkedin.com/in/liuyi1990/" target="_blank" rel="noopener noreferrer" class="social-link" title="LinkedIn" aria-label="Visit Yi Liu's LinkedIn profile">
                        <svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true">
                            <path fill="currentColor" d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/>
                        </svg>
                    </a>
                </div>
            </div>
        </aside>

        <!-- å³ä¾§ä¸»å†…å®¹åŒº -->
        <main class="main-content" role="main">
            <!-- å…³äºŽæˆ‘ -->
            <section id="about" class="content-section" aria-labelledby="about-title">
                <h2 id="about-title" class="section-title">About Me</h2>
                <div class="section-content">
                    <p>
                        I am a Senior Technical Manager at Baidu, leading the large model team at PaddlePaddle. We work across the full AI stack for large language modelsâ€”from infrastructure and development tools to algorithms and applicationsâ€”to power the ERNIE large model series, including ERNIE 3.5, 4.0, 4.5, and 5.0.
                    </p>
                    <p>
                        I have led over 10 open-source projects at Baidu that have collectively earned 150,000+ GitHub stars, such as PaddleOCR, ERNIE, PaddleFormers, and PaddleX, among others. These projects power critical AI applications worldwideâ€”from multilingual document understanding to large language model training and deployment at scale.
                    </p>
                    <p>
                        My technical expertise spans computer vision, vision-language models, large language models, and autonomous driving. I hold a Ph.D. from Nanyang Technological University, Singapore (2017) and a B.Eng. from Harbin Institute of Technology, China (2013). Before joining Baidu in 2018, I was a Data Scientist at HP Labs Singapore. At Baidu, I have also collaborated extensively with the Apollo team on autonomous driving technologies, contributing to Robotaxi and AD 2.0â€”an experience I deeply value.
                    </p>
                    <div class="hiring-callout">
                        <div class="hiring-icon">ðŸ”¥</div>
                        <div class="hiring-content">
                            <h3 class="hiring-title">We're Hiring!</h3>
                            <p class="hiring-text">We are always looking for talented interns and full-time engineers to join our team in areas including: Large Language Models, Reinforcement Learning, Multimodal Algorithms, Computer Vision, and LLM Agent Applications. If you are interested, please feel free to send your resume to my email.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- æœ€æ–°åŠ¨æ€ -->
            <section id="news" class="content-section" aria-labelledby="news-title">
                <h2 id="news-title" class="section-title">News</h2>
                <div class="section-content">
                    <ul class="news-list">
                        <li class="news-item">
                            <span class="news-date">Jan 2026</span>
                            <span class="news-text">We open-sourced <a href="https://arxiv.org/abs/2601.21957" target="_blank" rel="noopener noreferrer">PaddleOCR-VL-1.5</a>: A 0.9B lightweight VLM delivering SOTA document parsing and text spotting, leading across six scenarios. It adds new text spotting and seal recognition, supports 111 languages.</span>
                        </li>
                        <li class="news-item">
                            <span class="news-date">Jan 2026</span>
                            <span class="news-text">We released <a href="https://github.com/PaddlePaddle/Paddleformers" target="_blank" rel="noopener noreferrer">PaddleFormers 1.0</a>: It supports training for LLMs and VLMs, with extreme performance optimizations for key models like DeepSeek-V3 and GLM-4.5-Air, significantly outperforming Megatron-LM in training speed.</span>
                        </li>
                        <li class="news-item">
                            <span class="news-date">Dec 2025</span>
                            <span class="news-text">We officially launched <a href="https://www.paddleocr.com" target="_blank" rel="noopener noreferrer">PaddleOCR.com</a> , offering free hands-on experience with advanced document parsing capabilities.</span>
                        </li>
                        <li class="news-item">
                            <span class="news-date">Nov 2025</span>
                            <span class="news-text">We open-sourced <a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking" target="_blank" rel="noopener noreferrer">ERNIE-4.5-VL-28B-A3B-Thinking</a>: a lightweight VLM (3B active) matching flagship models, excelling in visual reasoning, STEM solving, visual grounding, and video understanding with tool utilization.</span>
                        </li>
                        <li class="news-item">
                            <span class="news-date">Oct 2025</span>
                            <span class="news-text">We open-sourced <a href="https://arxiv.org/abs/2510.14528" target="_blank" rel="noopener noreferrer">PaddleOCR-VL</a>: a SOTA 0.9B VLM for document parsing, supporting 109 languages and recognizing complex elements (text, tables, formulas, charts).</span>
                        </li>
                        <li class="news-item">
                            <span class="news-date">Sep 2025</span>
                            <span class="news-text">We open-sourced <a href="https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking" target="_blank" rel="noopener noreferrer">ERNIE-4.5-21B-A3B-Thinking</a>: an enhanced reasoning model with improved performance on logical reasoning, mathematics, science, coding, featuring tool usage and 128K long-context.</span>
                        </li>
                        <li class="news-item">
                            <span class="news-date">Aug 2025</span>
                            <span class="news-text">I gave a talk at IJCAI Conference on "PaddlePaddle Deep Learning Framework and its Support for Large Model Training and Inference".</span>
                        </li>
                        <li class="news-item">
                            <span class="news-date">Jun 2025</span>
                            <span class="news-text">We open-sourced <a href="https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf" target="_blank" rel="noopener noreferrer">ERNIE 4.5</a>: a family of large-scale multimodal models with 10 variants, including MoE models (47B/3B active, 424B total) and dense models (0.3B).</span>
                        </li>
                        <li class="news-item">
                            <span class="news-date">May 2025</span>
                            <span class="news-text">We open-sourced <a href="https://arxiv.org/abs/2507.05595" target="_blank" rel="noopener noreferrer">PaddleOCR 3.0</a>: featuring PP-OCRv5 (high-accuracy text recognition), PP-StructureV3 (general-purpose document parsing), and PP-ChatOCRv4 (intelligent document understanding).</span>
                        </li>
                    </ul>
                </div>
            </section>

            <!-- æ ¸å¿ƒé¡¹ç›® -->
            <section id="projects" class="content-section" aria-labelledby="projects-title">
                <h2 id="projects-title" class="section-title">Key Open-Source Projects</h2>
                <div class="section-content">
                    <div class="project-grid">
                        <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">
                                PaddleOCR
                                <span class="project-status status-active">Active</span>
                            </h3>
                            <div class="project-stats">
                                <a href="https://github.com/PaddlePaddle/PaddleOCR" target="_blank" rel="noopener noreferrer" class="github-link">
                                    <svg class="github-logo" viewBox="0 0 16 16" width="20" height="20" fill="currentColor">
                                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
                                    </svg>
                                    <span class="stat-number">62.9k</span>
                                </a>
                            </div>
                        </div>
                        <p class="project-description">
                            A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages with ultra-lightweight system, powering popular projects like Umi-OCR, OmniParser, MinerU, and RAGFlow.
                        </p>
                    </div>

                    <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">
                                PaddleFormers
                                <span class="project-status status-active">Active</span>
                            </h3>
                            <div class="project-stats">
                                <a href="https://github.com/PaddlePaddle/PaddleFormers" target="_blank" rel="noopener noreferrer" class="github-link">
                                    <svg class="github-logo" viewBox="0 0 16 16" width="20" height="20" fill="currentColor">
                                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
                                    </svg>
                                    <span class="stat-number">12.9k</span>
                                </a>
                            </div>
                        </div>
                        <p class="project-description">
                            An easy-to-use library of pre-trained large language models based on PaddlePaddle. Implements 4D parallel strategies through unified Trainer API, supporting SFT/DPO paradigms and integrating PEFT, MergeKit, and Quantization APIs for efficient LLM development.
                        </p>
                        </div>

                        <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">
                                ERNIE
                                <span class="project-status status-active">Active</span>
                            </h3>
                            <div class="project-stats">
                                <a href="https://github.com/PaddlePaddle/ERNIE" target="_blank" rel="noopener noreferrer" class="github-link">
                                    <svg class="github-logo" viewBox="0 0 16 16" width="20" height="20" fill="currentColor">
                                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
                                    </svg>
                                    <span class="stat-number">7.5k</span>
                                </a>
                            </div>
                        </div>
                        <p class="project-description">
                            Official repository for ERNIE 4.5, featuring both MoE and Dense models across LLMs and multimodal architectures. Provides end-to-end development pipeline for training, compression, and inference, supporting full-cycle industrial deployment.
                        </p>
                        </div>

                        <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">
                                PaddleX
                                <span class="project-status status-active">Active</span>
                            </h3>
                            <div class="project-stats">
                                <a href="https://github.com/PaddlePaddle/PaddleX" target="_blank" rel="noopener noreferrer" class="github-link">
                                    <svg class="github-logo" viewBox="0 0 16 16" width="20" height="20" fill="currentColor">
                                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
                                    </svg>
                                    <span class="stat-number">5.9k</span>
                                </a>
                            </div>
                        </div>
                        <p class="project-description">
                            All-in-One low-code development tool for AI models built on PaddlePaddle. Integrates over 200 ready-to-use pre-trained models covering OCR, object detection, and time series forecasting, supporting complete workflow from training to deployment.
                        </p>
                        </div>

                        <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">
                                PaddleDetection
                            </h3>
                            <div class="project-stats">
                                <a href="https://github.com/PaddlePaddle/PaddleDetection" target="_blank" rel="noopener noreferrer" class="github-link">
                                    <svg class="github-logo" viewBox="0 0 16 16" width="20" height="20" fill="currentColor">
                                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
                                    </svg>
                                    <span class="stat-number">13.1k</span>
                                </a>
                            </div>
                        </div>
                        <p class="project-description">
                            End-to-end object detection toolkit providing 30+ algorithms and 250+ pre-trained models. Supports object detection, instance segmentation, keypoint detection, and multiple object tracking with complete pipeline from development to deployment.
                        </p>
                        </div>

                        <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">
                                PaddleNLP
                            </h3>
                            <div class="project-stats">
                                <a href="https://github.com/PaddlePaddle/PaddleNLP" target="_blank" rel="noopener noreferrer" class="github-link">
                                    <svg class="github-logo" viewBox="0 0 16 16" width="20" height="20" fill="currentColor">
                                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
                                    </svg>
                                    <span class="stat-number">12.3k</span>
                                </a>
                            </div>
                        </div>
                        <p class="project-description">
                            Easy-to-use NLP library with 45+ architectures and 500+ pretrained models. Supports wide-range of tasks from research to industrial applications including Neural Search, Question Answering, Information Extraction, and Sentiment Analysis.
                        </p>
                    </div>

                    <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">
                                PaddleSeg
                            </h3>
                            <div class="project-stats">
                                <a href="https://github.com/PaddlePaddle/PaddleSeg" target="_blank" rel="noopener noreferrer" class="github-link">
                                    <svg class="github-logo" viewBox="0 0 16 16" width="20" height="20" fill="currentColor">
                                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
                                    </svg>
                                    <span class="stat-number">9.2k</span>
                                </a>
                            </div>
                        </div>
                        <p class="project-description">
                            Easy-to-use image segmentation library providing 45+ models and 150+ pre-trained models. Supports Semantic Segmentation, Interactive Segmentation, Panoptic Segmentation, Image Matting, and 3D Segmentation with complete flow from labeling to deployment.
                        </p>
                    </div>

                    <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">
                                PaddleClas
                            </h3>
                            <div class="project-stats">
                                <a href="https://github.com/PaddlePaddle/PaddleClas" target="_blank" rel="noopener noreferrer" class="github-link">
                                    <svg class="github-logo" viewBox="0 0 16 16" width="20" height="20" fill="currentColor">
                                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
                                    </svg>
                                    <span class="stat-number">5.8k</span>
                                </a>
                            </div>
                        </div>
                        <p class="project-description">
                            Comprehensive toolkit for image classification and recognition. Encompasses advanced algorithms including PP-HGNet, PP-LCNetv2, and PP-LCNet, providing 35 series with 164 ImageNet pre-trained models for industrial and academic applications.
                        </p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- å­¦æœ¯è®ºæ–‡ -->
            <section id="publications" class="content-section" aria-labelledby="publications-title">
                <h2 id="publications-title" class="section-title">Publications</h2>
                <div class="section-content">

                    <!-- Preprints -->
                    <div class="publication-category">
                        <h3 class="publication-category-title collapsible-title collapsed" data-target="arxiv-papers" role="button" aria-expanded="false" aria-controls="arxiv-papers" tabindex="0">
                            arXiv Papers <span class="collapse-icon" aria-hidden="true">â–¼</span>
                        </h3>

                        <div class="publication-list collapsed" id="arxiv-papers" role="region" aria-labelledby="arxiv-papers">
                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A18]</span> <a href="https://arxiv.org/abs/2510.14528" target="_blank" rel="noopener noreferrer" class="publication-title-link">PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model</a>
                                <span class="publication-venue-inline">2025</span>
                            </h3>
                            <p class="publication-authors">
                                Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Handong Zheng, Jing Zhang, Jun Zhang, <strong>Yi Liu</strong>, Dianhai Yu, Yanjun Ma
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A17]</span> <a href="https://arxiv.org/abs/2507.05595" target="_blank" rel="noopener noreferrer" class="publication-title-link">PaddleOCR 3.0 Technical Report</a>
                                <span class="publication-venue-inline">2025</span>
                            </h3>
                            <p class="publication-authors">
                                Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, <strong>Yi Liu</strong>, Dianhai Yu, Yanjun Ma
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A16]</span> <a href="https://arxiv.org/abs/2503.16983" target="_blank" rel="noopener noreferrer" class="publication-title-link">Enabling Versatile Controls for Video Diffusion Models</a>
                                <span class="publication-venue-inline">2025</span>
                            </h3>
                            <p class="publication-authors">
                                Xu Zhang, Hao Zhou, Haoming Qin, Xiaobin Lu, Jiaxing Yan, Guanzhong Wang, Zeyu Chen, <strong>Yi Liu</strong>
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A15]</span> <a href="https://arxiv.org/abs/2506.18023" target="_blank" rel="noopener noreferrer" class="publication-title-link">PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding</a>
                                <span class="publication-venue-inline">2025</span>
                            </h3>
                            <p class="publication-authors">
                                Kui Huang, Xinrong Chen, Wenyu Lv, Jincheng Liao, Guanzhong Wang, <strong>Yi Liu</strong>
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A14]</span> <a href="https://arxiv.org/abs/2503.04065" target="_blank" rel="noopener noreferrer" class="publication-title-link">PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks</a>
                                <span class="publication-venue-inline">2025</span>
                            </h3>
                            <p class="publication-authors">
                                Feng Ni, Kui Huang, Yao Lu, Wenyu Lv, Guanzhong Wang, Zeyu Chen, <strong>Yi Liu</strong>
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A13]</span> <a href="https://arxiv.org/abs/2503.18382" target="_blank" rel="noopener noreferrer" class="publication-title-link">PP-FormulaNet: Bridging Accuracy and Efficiency in Advanced Formula Recognition</a>
                                <span class="publication-venue-inline">2025</span>
                            </h3>
                            <p class="publication-authors">
                                Hongen Liu, Cheng Cui, Yuning Du, <strong>Yi Liu</strong>, Gang Pan
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A12]</span> <a href="https://arxiv.org/abs/2503.17213" target="_blank" rel="noopener noreferrer" class="publication-title-link">PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction</a>
                                <span class="publication-venue-inline">2025</span>
                            </h3>
                            <p class="publication-authors">
                                Ting Sun, Cheng Cui, Yuning Du, <strong>Yi Liu</strong>
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A11]</span> <a href="https://arxiv.org/abs/2405.19194" target="_blank" rel="noopener noreferrer" class="publication-title-link">LOGO: Video Text Spotting with Language Collaboration and Glyph Perception Model</a>
                                <span class="publication-venue-inline">2024</span>
                            </h3>
                            <p class="publication-authors">
                                Hongen Liu, Di Sun, Jiahao Wang, <strong>Yi Liu</strong>, Gang Pan
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A10]</span> <a href="https://arxiv.org/abs/2407.17140" target="_blank" rel="noopener noreferrer" class="publication-title-link">RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer</a>
                                <span class="publication-venue-inline">2024</span>
                            </h3>
                            <p class="publication-authors">
                                Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, <strong>Yi Liu</strong>
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A9]</span> <a href="https://arxiv.org/abs/2304.08069" target="_blank" rel="noopener noreferrer" class="publication-title-link">DETRs Beat YOLOs on Real-time Object Detection</a>
                                <span class="publication-venue-inline">2023</span>
                            </h3>
                            <p class="publication-authors">
                                Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang, Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, <strong>Yi Liu</strong>
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A8]</span> <a href="https://arxiv.org/abs/2304.05152" target="_blank" rel="noopener noreferrer" class="publication-title-link">PP-MobileSeg: Explore the Fast and Accurate Semantic Segmentation Model on Mobile Devices</a>
                                <span class="publication-venue-inline">2023</span>
                            </h3>
                            <p class="publication-authors">
                                Shiyu Tang, Ting Sun, Juncai Peng, Guowei Chen, Yuying Hao, Manhui Lin, Zhihong Xiao, Jiangbin You, <strong>Yi Liu</strong>
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A7]</span> <a href="https://arxiv.org/abs/2211.02386" target="_blank" rel="noopener noreferrer" class="publication-title-link">PP-YOLOE-R: An Efficient Anchor-Free Rotated Object Detector</a>
                                <span class="publication-venue-inline">2022</span>
                            </h3>
                            <p class="publication-authors">
                                Xinxin Wang, Guanzhong Wang, Qingqing Dang, <strong>Yi Liu</strong>, Xiaoguang Hu, Dianhai Yu
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A6]</span> <a href="https://arxiv.org/abs/2210.10984" target="_blank" rel="noopener noreferrer" class="publication-title-link">RAIS: Robust and Accurate Interactive Segmentation via Continual Learning</a>
                                <span class="publication-venue-inline">2022</span>
                            </h3>
                            <p class="publication-authors">
                                Yuying Hao, <strong>Yi Liu</strong>, Juncai Peng, Haoyi Xiong, Guowei Chen, Shiyu Tang, Zeyu Chen, Baohua Lai
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A5]</span> <a href="https://arxiv.org/abs/2210.05391" target="_blank" rel="noopener noreferrer" class="publication-title-link">PP-StructureV2: A Stronger Document Analysis System</a>
                                <span class="publication-venue-inline">2022</span>
                            </h3>
                            <p class="publication-authors">
                                Chenxia Li, Ruoyu Guo, Jun Zhou, Mengtao An, Yuning Du, Lingfeng Zhu, <strong>Yi Liu</strong>, Xiaoguang Hu, Dianhai Yu
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A4]</span> <a href="https://arxiv.org/abs/2210.08788" target="_blank" rel="noopener noreferrer" class="publication-title-link">EISeg: An Efficient Interactive Segmentation Tool based on PaddlePaddle</a>
                                <span class="publication-venue-inline">2022</span>
                            </h3>
                            <p class="publication-authors">
                                Yuying Hao, <strong>Yi Liu</strong>, Yizhou Chen, Lin Han, Juncai Peng, Shiyu Tang, Guowei Chen, Zewu Wu, Zeyu Chen, Baohua Lai
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A3]</span> <a href="https://arxiv.org/abs/2204.09433" target="_blank" rel="noopener noreferrer" class="publication-title-link">PP-Matting: High-Accuracy Natural Image Matting</a>
                                <span class="publication-venue-inline">2022</span>
                            </h3>
                            <p class="publication-authors">
                                Guowei Chen, <strong>Yi Liu</strong>, Jian Wang, Juncai Peng, Yuying Hao, Lutao Chu, Shiyu Tang, Zewu Wu, Zeyu Chen, Zhiliang Yu, Yuning Du, Qingqing Dang, Xiaoguang Hu, Dianhai Yu
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A2]</span> <a href="https://arxiv.org/abs/2204.02681" target="_blank" rel="noopener noreferrer" class="publication-title-link">PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model</a>
                                <span class="publication-venue-inline">2022</span>
                            </h3>
                            <p class="publication-authors">
                                Juncai Peng, <strong>Yi Liu</strong>, Shiyu Tang, Yuying Hao, Lutao Chu, Guowei Chen, Zewu Wu, Zeyu Chen, Zhiliang Yu, Yuning Du, Qingqing Dang, Baohua Lai, Qiwen Liu, Xiaoguang Hu, Dianhai Yu, Yanjun Ma
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[A1]</span> <a href="https://arxiv.org/abs/2101.06175" target="_blank" rel="noopener noreferrer" class="publication-title-link">PaddleSeg: A High-Efficient Development Toolkit for Image Segmentation</a>
                                <span class="publication-venue-inline">2021</span>
                            </h3>
                            <p class="publication-authors">
                                <strong>Yi Liu</strong>, Lutao Chu, Guowei Chen, Zewu Wu, Zeyu Chen, Baohua Lai, Yuying Hao
                            </p>
                        </div>
                        </div>
                    </div>

                    <!-- Conference Papers -->
                    <div class="publication-category">
                        <h3 class="publication-category-title collapsible-title collapsed" data-target="conference-papers" role="button" aria-expanded="false" aria-controls="conference-papers" tabindex="0">
                            Conference Papers <span class="collapse-icon" aria-hidden="true">â–¼</span>
                        </h3>

                        <div class="publication-list collapsed" id="conference-papers" role="region" aria-labelledby="conference-papers">
                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C20]</span> <a href="https://arxiv.org/abs/2508.00412" target="_blank" rel="noopener noreferrer" class="publication-title-link">Sortblock: Similarity-Aware Feature Reuse for Diffusion Model</a>
                                <span class="publication-venue-inline">AAAI 2026</span>
                            </h3>
                            <p class="publication-authors">
                                Hanqi Chen, Xu Zhang, Xiaoliu Guan, Lielin Jiang, Guanzhong Wang, Zeyu Chen, <strong>Yi Liu</strong>
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C19]</span> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32223" target="_blank" rel="noopener noreferrer" class="publication-title-link">SUTrack: Towards Simple and Unified Single Object Tracking</a>
                                <span class="publication-venue-inline">AAAI 2025</span>
                            </h3>
                            <p class="publication-authors">
                                Xin Chen, Ben Kang, Wanting Geng, Jiawen Zhu, <strong>Yi Liu</strong>, Dong Wang, Huchuan Lu
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C18]</span> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32440" target="_blank" rel="noopener noreferrer" class="publication-title-link">Exploring Enhanced Contextual Information for Video-Level Object Tracking</a>
                                <span class="publication-venue-inline">AAAI 2025</span>
                            </h3>
                            <p class="publication-authors">
                                Ben Kang, Xin Chen, Simiao Lai, Yang Liu, <strong>Yi Liu</strong>, Dong Wang
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C17]</span> <a href="https://arxiv.org/abs/2309.13604" target="_blank" rel="noopener noreferrer" class="publication-title-link">Distribution-Aware Continual Test-Time Adaptation for Semantic Segmentation</a>
                                <span class="publication-venue-inline">ICRA 2024</span>
                            </h3>
                            <p class="publication-authors">
                                Jiayi Ni, Senqiao Yang, Ran Xu, Jiaming Liu, Xiaoqi Li, Wenyu Jiao, Zehui Chen, <strong>Yi Liu</strong>, Shanghang Zhang
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C16]</span> <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.html" target="_blank" rel="noopener noreferrer" class="publication-title-link">DETRs Beat YOLOs on Real-time Object Detection</a>
                                <span class="publication-venue-inline">CVPR 2024</span>
                            </h3>
                            <p class="publication-authors">
                                Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, <strong>Yi Liu</strong>, Jie Chen
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C15]</span> <a href="https://ieeexplore.ieee.org/document/10230371/" target="_blank" rel="noopener noreferrer" class="publication-title-link">Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-Based Video Classification Frameworks</a>
                                <span class="publication-venue-inline">ISBI 2023</span>
                            </h3>
                            <p class="publication-authors">
                                Yuxuan Zhang, Qingzhong Wang, Jiang Bian, <strong>Yi Liu</strong>, Yanwu Xu, Dejing Dou, Haoyi Xiong
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C14]</span> <a href="https://link.springer.com/chapter/10.1007/978-3-031-48593-0_11" target="_blank" rel="noopener noreferrer" class="publication-title-link">Context Matters: Cross-Domain Cell Detection in Histopathology Images via Contextual Regularization</a>
                                <span class="publication-venue-inline">MIUA 2023</span>
                            </h3>
                            <p class="publication-authors">
                                Ziqi Wen, Qingzhong Wang, Jiang Bian, Xuhong Li, <strong>Yi Liu</strong>, Haoyi Xiong
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C13]</span> <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Lightweight_Image_Super-Resolution_with_Superpixel_Token_Interaction_ICCV_2023_paper.html" target="_blank" rel="noopener noreferrer" class="publication-title-link">Lightweight Image Super-Resolution with Superpixel Token Interaction</a>
                                <span class="publication-venue-inline">ICCV 2023</span>
                            </h3>
                            <p class="publication-authors">
                                Aiping Zhang, Wenqi Ren, <strong>Yi Liu</strong>, Xiaochun Cao
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C12]</span> <a href="https://ieeexplore.ieee.org/document/9812325/" target="_blank" rel="noopener noreferrer" class="publication-title-link">Towards Efficient 3D Human Motion Prediction using Deformable Transformer-based Adversarial Network</a>
                                <span class="publication-venue-inline">ICRA 2022</span>
                            </h3>
                            <p class="publication-authors">
                                Hua Yu, Xuanzhe Fan, Yaqing Hou, <strong>Yi Liu</strong>, Cai Kang, Dongsheng Zhou, Qiang Zhang
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C11]</span> <a href="https://link.springer.com/chapter/10.1007/978-3-031-16452-1_15" target="_blank" rel="noopener noreferrer" class="publication-title-link">MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-Ray Images of Multiple Body Parts</a>
                                <span class="publication-venue-inline">MICCAI 2022</span>
                            </h3>
                            <p class="publication-authors">
                                Weibin Liao, Haoyi Xiong, Qingzhong Wang, Yan Mo, Xuhong Li, <strong>Yi Liu</strong>, Zeyu Chen, Siyu Huang, Dejing Dou
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C10]</span> <a href="https://openaccess.thecvf.com/content/WACV2022W/HADCV/html/Chu_PP-HumanSeg_Connectivity-Aware_Portrait_Segmentation_With_a_Large-Scale_Teleconferencing_Video_Dataset_WACVW_2022_paper.html" target="_blank" rel="noopener noreferrer" class="publication-title-link">PP-HumanSeg: Connectivity-Aware Portrait Segmentation With a Large-Scale Teleconferencing Video Dataset</a>
                                <span class="publication-venue-inline">WACV 2022</span>
                            </h3>
                            <p class="publication-authors">
                                Lutao Chu, <strong>Yi Liu</strong>, Zewu Wu, Shiyu Tang, Guowei Chen, Yuying Hao, Juncai Peng, Zhiliang Yu, Zeyu Chen, Baohua Lai, Haoyi Xiong
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C9]</span> <a href="https://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Hao_EdgeFlow_Achieving_Practical_Interactive_Segmentation_With_Edge-Guided_Flow_ICCVW_2021_paper.html" target="_blank" rel="noopener noreferrer" class="publication-title-link">EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow</a>
                                <span class="publication-venue-inline">ICCV 2021</span>
                            </h3>
                            <p class="publication-authors">
                                Yuying Hao, <strong>Yi Liu</strong>, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, Baohua Lai
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C8]</span> <a href="https://ieeexplore.ieee.org/document/7986840/" target="_blank" rel="noopener noreferrer" class="publication-title-link">A New Reconstruction Method in Gaze Estimation with Natural Head Movement</a>
                                <span class="publication-venue-inline">MVA 2017</span>
                            </h3>
                            <p class="publication-authors">
                                <strong>Yi Liu</strong>, Bu-Sung Lee, Andrzej Sluzek, Deepu Rajan, Martin J. McKeown
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C7]</span> <a href="https://link.springer.com/chapter/10.1007/978-3-319-48881-3_18" target="_blank" rel="noopener noreferrer" class="publication-title-link">Feasibility Analysis of Eye Typing with a Standard Webcam</a>
                                <span class="publication-venue-inline">ECCV Workshop 2016</span>
                            </h3>
                            <p class="publication-authors">
                                <strong>Yi Liu</strong>, Bu-Sung Lee, Andrzej Sluzek, Deepu Rajan, Martin J. McKeown
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C6]</span> <a href="https://dl.acm.org/doi/10.1145/2838739.2838813" target="_blank" rel="noopener noreferrer" class="publication-title-link">GazeTry: Swipe Text Typing Using Gaze</a>
                                <span class="publication-venue-inline">OzCHI 2015</span>
                            </h3>
                            <p class="publication-authors">
                                <strong>Yi Liu</strong>, Chi Zhang, Chonho Lee, Bu-Sung Lee, Alex Q. Chen
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C5]</span> <a href="https://ieeexplore.ieee.org/document/7423794/" target="_blank" rel="noopener noreferrer" class="publication-title-link">A Robust Recognition Approach in Eye-Based Dwell-Free Typing</a>
                                <span class="publication-venue-inline">IEEE PIC 2015</span>
                            </h3>
                            <p class="publication-authors">
                                <strong>Yi Liu</strong>, Bu-Sung Lee, Martin J. McKeown, Chonho Lee
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C4]</span> <a href="https://ieeexplore.ieee.org/document/7362195/" target="_blank" rel="noopener noreferrer" class="publication-title-link">Feasibility Analysis and Adaptive Thresholding for Mobile Applications Controlled by EEG Signals</a>
                                <span class="publication-venue-inline">EUSIPCO 2015</span>
                            </h3>
                            <p class="publication-authors">
                                Chonho Lee, Jiawei Chin, <strong>Yi Liu</strong>, Bu-Sung Lee, Martin J. McKeown
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C3]</span> <a href="https://ieeexplore.ieee.org/document/6918537/" target="_blank" rel="noopener noreferrer" class="publication-title-link">A Wavelet Entropy-Based Change Point Detection on Network Traffic: A Case Study of Heartbleed Vulnerability</a>
                                <span class="publication-venue-inline">IEEE CCTA 2014</span>
                            </h3>
                            <p class="publication-authors">
                                Chonho Lee, <strong>Yi Liu</strong>, Lim Hui Tan, Wei Goh, Bu-Sung Lee, Chai Kiat Yeo
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C2]</span> <a href="https://ieeexplore.ieee.org/document/6864446/" target="_blank" rel="noopener noreferrer" class="publication-title-link">A Motion Accuracy Evaluator Based on Body Parts Movement by MapReduce Video Processing</a>
                                <span class="publication-venue-inline">IEEE BHI 2014</span>
                            </h3>
                            <p class="publication-authors">
                                Chonho Lee, Yoshihiro Terada, <strong>Yi Liu</strong>, Bu-Sung Lee
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[C1]</span> <a href="https://ieeexplore.ieee.org/document/7001885/" target="_blank" rel="noopener noreferrer" class="publication-title-link">Analysis of Visually Guided Tracking Performance in Parkinson's Disease</a>
                                <span class="publication-venue-inline">IEEE e-Health 2014</span>
                            </h3>
                            <p class="publication-authors">
                                <strong>Yi Liu</strong>, Chonho Lee, Bu-Sung Lee, John Keith Robert Stevenson, Martin J. McKeown
                            </p>
                        </div>
                        </div>
                    </div>

                    <!-- Journal Papers -->
                    <div class="publication-category">
                        <h3 class="publication-category-title collapsible-title collapsed" data-target="journal-papers" role="button" aria-expanded="false" aria-controls="journal-papers" tabindex="0">
                            Journal Papers <span class="collapse-icon" aria-hidden="true">â–¼</span>
                        </h3>

                        <div class="publication-list collapsed" id="journal-papers" role="region" aria-labelledby="journal-papers">
                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[J7]</span> <a href="https://ieeexplore.ieee.org/document/10742428/" target="_blank" rel="noopener noreferrer" class="publication-title-link">DSDC-GCN: Decoupled Static-Dynamic Co-Occurrence Graph Convolutional Networks for Skeleton-Based Action Recognition</a>
                                <span class="publication-venue-inline">IEEE Transactions on Circuits and Systems for Video Technology 2025</span>
                            </h3>
                            <p class="publication-authors">
                                Tianming Zhuang, Zhen Qin, Yi Ding, Zhiguang Qin, Ji Geng, <strong>Yi Liu</strong>, Kim-Kwang Raymond Choo
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[J6]</span> <a href="https://ieeexplore.ieee.org/document/11021303/" target="_blank" rel="noopener noreferrer" class="publication-title-link">EGAvatar: Efficient GAN Inversion for Generalizable Head Avatar From Few-Shot Images</a>
                                <span class="publication-venue-inline">IEEE Transactions on Visualization and Computer Graphics 2025</span>
                            </h3>
                            <p class="publication-authors">
                                Hao-Pan Ren, Wei Duan, Wan-Yu Li, <strong>Yi Liu</strong>, Yu-Dong Guo, Shi-Sheng Huang, Ju-Yong Zhang, Hua Huang
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[J5]</span> <a href="https://arxiv.org/abs/2508.02240" target="_blank" rel="noopener noreferrer" class="publication-title-link">Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor</a>
                                <span class="publication-venue-inline">Knowledge-Based Systems 2025</span>
                            </h3>
                            <p class="publication-authors">
                                Xiaoliu Guan, Lielin Jiang, Hanqi Chen, Xu Zhang, Jiaxing Yan, Guanzhong Wang, <strong>Yi Liu</strong>, Zetao Zhang, Yu Wu
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[J4]</span> <a href="https://ieeexplore.ieee.org/document/10531186/" target="_blank" rel="noopener noreferrer" class="publication-title-link">MTPret: Improving X-Ray Image Analytics With Multitask Pretraining</a>
                                <span class="publication-venue-inline">IEEE Transactions on Artificial Intelligence 2024</span>
                            </h3>
                            <p class="publication-authors">
                                Weibin Liao, Haoyi Xiong, Qingzhong Wang, <strong>Yi Liu</strong>, Zeyu Chen, Qinghua Zheng, Dejing Dou
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[J3]</span> <a href="https://arxiv.org/abs/2207.03335" target="_blank" rel="noopener noreferrer" class="publication-title-link">Distilling Ensemble of Explanations for Weakly-Supervised Pre-Training of Image Segmentation Models</a>
                                <span class="publication-venue-inline">Machine Learning 2023</span>
                            </h3>
                            <p class="publication-authors">
                                Xuhong Li, Haoyi Xiong, <strong>Yi Liu</strong>, Dingfu Zhou, Zeyu Chen, Yaqing Wang, Dejing Dou
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[J2]</span> <a href="https://link.springer.com/article/10.1007/s00138-018-0988-4" target="_blank" rel="noopener noreferrer" class="publication-title-link">CamType: Assistive Text Entry Using Gaze with an Off-the-Shelf Webcam</a>
                                <span class="publication-venue-inline">Machine Vision and Applications 2019</span>
                            </h3>
                            <p class="publication-authors">
                                <strong>Yi Liu</strong>, Bu-Sung Lee, Deepu Rajan, Andrzej Sluzek, Martin J. McKeown
                            </p>
                        </div>

                        <div class="publication">
                            <h3 class="publication-title">
                                <span class="publication-number">[J1]</span> <a href="https://www.tandfonline.com/doi/abs/10.1080/10447318.2016.1151808" target="_blank" rel="noopener noreferrer" class="publication-title-link">Robust Eye-Based Dwell-Free Typing</a>
                                <span class="publication-venue-inline">International Journal of Humanâ€“Computer Interaction 2016</span>
                            </h3>
                            <p class="publication-authors">
                                <strong>Yi Liu</strong>, Bu-Sung Lee, Martin J. McKeown
                            </p>
                        </div>
                        </div>
                    </div>

                </div>
            </section>

            <!-- é¡µè„š -->
            <footer class="footer" role="contentinfo">
                <p>Last updated: February 2026</p>
            </footer>
        </main>
    </div>

    <!-- å®žæ—¶ GitHub Stars æ•°æ®èŽ·å–è„šæœ¬ -->
    <script>
        // é¡¹ç›®ä»“åº“æ˜ å°„è¡¨
        const projects = [
            { owner: 'PaddlePaddle', repo: 'PaddleOCR' },
            { owner: 'PaddlePaddle', repo: 'PaddleFormers' },
            { owner: 'PaddlePaddle', repo: 'ERNIE' },
            { owner: 'PaddlePaddle', repo: 'PaddleX' },
            { owner: 'PaddlePaddle', repo: 'PaddleDetection' },
            { owner: 'PaddlePaddle', repo: 'PaddleNLP' },
            { owner: 'PaddlePaddle', repo: 'PaddleSeg' },
            { owner: 'PaddlePaddle', repo: 'PaddleClas' }
        ];

        // æ ‡å‡†åŒ– star æ•°é‡æ ¼å¼
        function normalizeStarCount(shieldsValue) {
            if (!shieldsValue) return null;

            // shields.io è¿”å›žçš„æ ¼å¼: "62.9k", "13k", "12.9k" ç­‰
            // ç›´æŽ¥è¿”å›ž shields.io çš„å€¼ï¼Œä¿æŒåŽŸæœ‰æ ¼å¼
            return shieldsValue;
        }

        // é€šè¿‡ shields.io API èŽ·å– star æ•°
        async function fetchStarCount(owner, repo) {
            try {
                const response = await fetch(`https://img.shields.io/github/stars/${owner}/${repo}.json`, {
                    // æ·»åŠ è¶…æ—¶å’Œç¼“å­˜æŽ§åˆ¶
                    signal: AbortSignal.timeout(5000), // 5ç§’è¶…æ—¶
                    cache: 'default'
                });

                if (!response.ok) {
                    console.warn(`GitHub API rate limit or error for ${owner}/${repo}`);
                    return null;
                }

                const data = await response.json();

                // æ£€æŸ¥æ˜¯å¦è¿”å›žäº†é”™è¯¯ä¿¡æ¯
                if (data.message && data.message.includes('Unable to select')) {
                    console.warn(`GitHub token pool exhausted for ${owner}/${repo}`);
                    return null;
                }

                // shields.io è¿”å›žæ ¼å¼åŒ–åŽçš„å€¼ï¼Œå¦‚ "62.9k" æˆ– "13k"
                return normalizeStarCount(data.value || data.message);
            } catch (error) {
                // é™é»˜å¤„ç†é”™è¯¯ï¼Œä¸åœ¨æŽ§åˆ¶å°æ˜¾ç¤ºï¼ˆé¿å…è¿‡å¤šé”™è¯¯ä¿¡æ¯ï¼‰
                return null;
            }
        }

        // æ›´æ–°é¡µé¢ä¸Šçš„ star æ•°
        function updateStarCount(owner, repo, starCount) {
            // æ‰¾åˆ°å¯¹åº”çš„ GitHub é“¾æŽ¥
            const links = document.querySelectorAll('.github-link');
            links.forEach(link => {
                const href = link.getAttribute('href');
                if (href && href.includes(`${owner}/${repo}`)) {
                    const statNumber = link.querySelector('.stat-number');
                    if (statNumber && starCount) {
                        // æ·»åŠ æ·¡å‡ºæ•ˆæžœ
                        statNumber.style.transition = 'opacity 0.3s ease';
                        statNumber.style.opacity = '0';

                        // 300ms åŽæ›´æ–°æ•°å€¼å¹¶æ·¡å…¥
                        setTimeout(() => {
                            statNumber.textContent = starCount;
                            statNumber.style.opacity = '1';
                        }, 300);
                    }
                }
            });
        }

        // åŠ è½½æ‰€æœ‰é¡¹ç›®çš„ star æ•°
        async function loadAllStars() {
            for (const project of projects) {
                const starCount = await fetchStarCount(project.owner, project.repo);
                if (starCount) {
                    updateStarCount(project.owner, project.repo, starCount);
                }
                // æ·»åŠ å°å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                await new Promise(resolve => setTimeout(resolve, 100));
            }
        }

        // é¡µé¢åŠ è½½å®ŒæˆåŽæ‰§è¡Œ
        if (document.readyState === 'loading') {
            document.addEventListener('DOMContentLoaded', loadAllStars);
        } else {
            loadAllStars();
        }
    </script>

    <!-- å¯¼èˆªæ é«˜äº®è„šæœ¬ -->
    <script>
        // å¯¼èˆªæ æ¿€æ´»çŠ¶æ€ç®¡ç†
        function updateActiveNav() {
            const sections = document.querySelectorAll('.content-section');
            const navLinks = document.querySelectorAll('.nav-link');

            let currentSection = '';
            const scrollPosition = window.scrollY + 100; // åç§»é‡ï¼Œæå‰è§¦å‘

            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                const sectionHeight = section.clientHeight;

                if (scrollPosition >= sectionTop && scrollPosition < sectionTop + sectionHeight) {
                    currentSection = section.getAttribute('id');
                }
            });

            navLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === `#${currentSection}`) {
                    link.classList.add('active');
                }
            });
        }

        // ç›‘å¬æ»šåŠ¨äº‹ä»¶
        window.addEventListener('scroll', updateActiveNav);

        // é¡µé¢åŠ è½½æ—¶æ‰§è¡Œä¸€æ¬¡
        window.addEventListener('load', updateActiveNav);
    </script>

    <!-- å‘å¸ƒæŠ˜å åŠŸèƒ½è„šæœ¬ -->
    <script>
        // åˆå§‹åŒ–æŠ˜å åŠŸèƒ½
        function initCollapsibles() {
            const collapsibleTitles = document.querySelectorAll('.collapsible-title');

            collapsibleTitles.forEach(title => {
                // æ”¯æŒç‚¹å‡»å’Œé”®ç›˜æ“ä½œ
                title.addEventListener('click', toggleCollapsible);
                title.addEventListener('keydown', function(e) {
                    if (e.key === 'Enter' || e.key === ' ') {
                        e.preventDefault();
                        toggleCollapsible.call(this);
                    }
                });
            });
        }

        function toggleCollapsible() {
            // èŽ·å–ç›®æ ‡åˆ—è¡¨çš„ID
            const targetId = this.getAttribute('data-target');
            const targetList = document.getElementById(targetId);

            if (!targetList) return;

            // åˆ‡æ¢æŠ˜å çŠ¶æ€
            const isCollapsed = this.classList.toggle('collapsed');
            targetList.classList.toggle('collapsed');

            // æ›´æ–° ARIA å±žæ€§
            this.setAttribute('aria-expanded', isCollapsed ? 'false' : 'true');
        }

        // é¡µé¢åŠ è½½å®ŒæˆåŽåˆå§‹åŒ–
        if (document.readyState === 'loading') {
            document.addEventListener('DOMContentLoaded', initCollapsibles);
        } else {
            initCollapsibles();
        }
    </script>

    <!-- ä¸»é¢˜åˆ‡æ¢åŠŸèƒ½è„šæœ¬ -->
    <script>
        // ä¸»é¢˜åˆ‡æ¢åŠŸèƒ½
        (function() {
            const themeToggle = document.getElementById('themeToggle');
            const sunIcon = document.getElementById('sunIcon');
            const moonIcon = document.getElementById('moonIcon');
            const html = document.documentElement;

            // ä»Ž localStorage èŽ·å–ä¿å­˜çš„ä¸»é¢˜ï¼Œé»˜è®¤ä¸ºæš—é»‘ä¸»é¢˜
            const savedTheme = localStorage.getItem('theme') || 'dark';

            // åº”ç”¨ä¸»é¢˜
            function applyTheme(theme) {
                if (theme === 'light') {
                    html.setAttribute('data-theme', 'light');
                    sunIcon.style.display = 'none';
                    moonIcon.style.display = 'block';
                } else {
                    html.removeAttribute('data-theme');
                    sunIcon.style.display = 'block';
                    moonIcon.style.display = 'none';
                }
                localStorage.setItem('theme', theme);
            }

            // åˆå§‹åŒ–ä¸»é¢˜
            applyTheme(savedTheme);

            // åˆ‡æ¢ä¸»é¢˜
            themeToggle.addEventListener('click', function() {
                const currentTheme = html.getAttribute('data-theme');
                const newTheme = currentTheme === 'light' ? 'dark' : 'light';

                // æ·»åŠ æ—‹è½¬åŠ¨ç”»
                themeToggle.classList.add('rotating');

                // åˆ‡æ¢ä¸»é¢˜
                applyTheme(newTheme);

                // ç§»é™¤åŠ¨ç”»ç±»
                setTimeout(() => {
                    themeToggle.classList.remove('rotating');
                }, 500);
            });

            // é”®ç›˜æ”¯æŒ
            themeToggle.addEventListener('keydown', function(e) {
                if (e.key === 'Enter' || e.key === ' ') {
                    e.preventDefault();
                    this.click();
                }
            });
        })();
    </script>
</body>
</html>
